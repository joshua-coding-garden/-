import json
import os
import time
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter  # æ–°å¢é€™å€‹åˆ‡èœæ©Ÿ
from tqdm import tqdm

# ================= è¨­å®šå€ =================
# ä½ çš„ Windows IP (æ ¹æ“šä½ å‰›å‰› curl æˆåŠŸçš„çµæœ)
OLLAMA_URL = "http://172.18.112.1:11434"

DATA_FILES = ["health.json", "medical.json"]
DB_PATH = "./chroma_db"
BATCH_SIZE = 500  # æ‰¹æ¬¡å¤§å°
# =========================================

def main():
    print(f"ç›®æ¨™ Ollama IP: {OLLAMA_URL}")
    
    # --- 1. è®€å–æ•¸æ“š ---

    # --- 1. è®€å–æ•¸æ“š ---
    raw_data = []

    for file in DATA_FILES:
        if not os.path.exists(file):
            print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ {file}ï¼Œè·³é")
            continue

        with open(file, "r", encoding="utf-8") as f:
            data = json.load(f)

        print(f"ğŸ“„ è¼‰å…¥ {file}ï¼Œå…± {len(data)} ç­†")

        # å¹«æ¯ä¸€ç­†è³‡æ–™åŠ ä¸Šä¾†æº
        for d in data:
            d["source"] = file
            raw_data.append(d)

    total_records = len(raw_data)
    print(f"âœ… ç¸½å…±åˆä½µ {total_records} ç­†æ•¸æ“š")




    # --- 2. åˆå§‹åŒ–å·¥å…· ---
    #text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    
    embeddings = OllamaEmbeddings(
        model="nomic-embed-text",
        base_url=OLLAMA_URL
    )

    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name="medical_rag"
    )

    text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=100
    )

    # --- 3. è®€å–é€²åº¦ç´€éŒ„ (æ–·é»çºŒå‚³é‚è¼¯) ---
    CHECKPOINT_FILE = "checkpoint_index.txt"
    start_index = 0
    
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, "r") as f:
            try:
                start_index = int(f.read().strip())
                print(f"ğŸ“Œ ç™¼ç¾ä¸Šæ¬¡é€²åº¦ï¼Œå°‡å¾ç¬¬ {start_index} ç­†é–‹å§‹ç¹¼çºŒè™•ç†...")
            except ValueError:
                start_index = 0
    
    # å¦‚æœå·²ç¶“å…¨éƒ¨è·‘å®Œ
    if start_index >= total_records:
        print("âœ… æ‰€æœ‰æ•¸æ“šå·²è™•ç†å®Œç•¢ï¼")
        return

    print(f"é–‹å§‹è™•ç†å‘é‡åŒ–...")

    batch_docs = []
    
    # --- 4. ä¸»è¿´åœˆ (åŠ å…¥ tqdm é€²åº¦æ¢) ---
    # initial=start_index è®“é€²åº¦æ¢é¡¯ç¤ºæ­£ç¢ºçš„ç™¾åˆ†æ¯”
    for i, item in tqdm(enumerate(raw_data), total=total_records, initial=start_index, desc="Processing"):
        
        # [é—œéµ] è·³éå·²ç¶“è™•ç†éçš„è³‡æ–™
        if i < start_index:
            continue

        q = item.get('question', '')
        a = item.get('answer', '')
        content = f"å•é¡Œ: {q}\nå›ç­”: {a}"
        
        original_doc = Document(
            page_content=content, 
            metadata={
                "source": item.get("source", "merged_json"),
                "id": str(i),
                "original_q": q
            }
        )

        splits = [original_doc]
        if len(content) < 800:   # çŸ­æ–‡æœ¬ï¼šç›´æ¥ç”¨
            batch_docs.append(original_doc)
        else:                    # é•·æ–‡æœ¬ï¼šåˆ‡ chunk
            splits = text_splitter.split_documents([original_doc])
            batch_docs.extend(splits)


        # ç´¯ç©æ»¿ BATCH_SIZE å°±å¯«å…¥
        if len(batch_docs) >= BATCH_SIZE:
            try:
                vectorstore.add_documents(batch_docs)
                batch_docs = [] # æ¸…ç©º
                
                # [é—œéµ] å¯«å…¥æˆåŠŸå¾Œï¼Œç«‹åˆ»æ›´æ–°é€²åº¦æª”
                # ç´€éŒ„ i + 1ï¼Œä»£è¡¨ä¸‹æ¬¡å¾ä¸‹ä¸€ç­†é–‹å§‹
                with open(CHECKPOINT_FILE, "w") as f:
                    f.write(str(i + 1))
                    
            except Exception as e:
                print(f"\n[è­¦å‘Š] å¯«å…¥å¤±æ•—: {e}")
                # é€™è£¡ä¸æ›´æ–°é€²åº¦æª”ï¼Œä¸‹æ¬¡é‡è·‘æœƒé‡è©¦é€™æ‰¹

    # --- 5. è™•ç†å‰©ä¸‹çš„å°¾æ•¸ ---
    if batch_docs:
        try:
            vectorstore.add_documents(batch_docs)
            with open(CHECKPOINT_FILE, "w") as f:
                f.write(str(total_records))
        except Exception as e:
            print(f"\n[è­¦å‘Š] æœ€å¾Œä¸€æ‰¹å¯«å…¥å¤±æ•—: {e}")

    print("\n==============================")
    print("ğŸ‰ è³‡æ–™åº«å»ºç«‹/æ›´æ–°å®Œç•¢ï¼")
    print(f"è³‡æ–™å·²å­˜å…¥ {DB_PATH}")

if __name__ == "__main__":
    main()
