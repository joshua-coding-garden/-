from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
from opencc import OpenCC
from rag_core import initialize_rag_system

# 1. å»ºç«‹ Flask App (é€™ä¸€è¡Œå°±æ˜¯æ‚¨ç¼ºå°‘çš„)
app = Flask(__name__)
CORS(app)  # å…è¨±è·¨åŸŸè«‹æ±‚

# ================= è¨­å®šå€ =================
WINDOWS_IP = "172.18.112.1"  # è«‹ç¢ºèªé€™æ˜¯å¦ç‚ºæ‚¨ç•¶å‰çš„ IP
OLLAMA_API_URL = f"http://{WINDOWS_IP}:11434/api/generate"
MODEL_NAME = "qwen2.5:14b"
cc = OpenCC('s2twp')

# å…¨åŸŸè®Šæ•¸å­˜æ”¾ RAG å¼•æ“
rag_engine = None

def get_rag_engine():
    global rag_engine
    if rag_engine is None:
        print("ğŸš€ [Server] æ­£åœ¨åˆå§‹åŒ– RAG å¼•æ“...")
        rag_engine = initialize_rag_system()
    return rag_engine

# 2. ä¿®æ”¹å¾Œçš„ä¸Šä¸‹æ–‡ç²å–å‡½å¼ (åŒ…å«åˆ†æ•¸è™•ç†)
def get_knowledge_context(engine, user_question):
    results = engine.search(user_question, k=3) 
    context_str = ""
    source_data = [] # ç”¨ä¾†å­˜çµ¦å‰ç«¯é¡¯ç¤ºçš„è³‡æ–™

    if not results:
        return "", []

    for i, res in enumerate(results):
        doc = res['doc']
        score = res['score'] 
        
        # éæ¿¾ä½åˆ†
        if score < 0.35: continue 
        
        # 1. æ‹¼æ¹Šçµ¦ LLM çœ‹çš„å­—ä¸²
        context_str += f"ã€åƒè€ƒæ–‡ç» {i+1}ã€‘\nå…§å®¹: {doc['a']}\n\n"
        
        # 2. æº–å‚™çµ¦å‰ç«¯é¡¯ç¤ºçš„è©³ç´°è³‡æ–™
        source_data.append({
            "id": i + 1,
            "content": doc['a'],
            "score": round(score * 10, 2) # å°‡åˆ†æ•¸è½‰æ›ç‚ºæ˜“è®€æ ¼å¼
        })
        
    return context_str, source_data

@app.route('/')
def index():
    return "<h1>ğŸš€ AI é†«ç™‚ä¼ºæœå™¨é‹ä½œä¸­ï¼</h1><p>è«‹æ‰“é–‹ index.html ä¾†ä½¿ç”¨èŠå¤©ä»‹é¢ã€‚</p>"

# 3. ä¿®æ”¹å¾Œçš„ API æ¥å£
@app.route('/ask', methods=['POST'])
def ask_question():
    data = request.json
    user_question = data.get('question', '')
    
    if not user_question:
        return jsonify({"answer": "è«‹è¼¸å…¥å•é¡Œ"}), 400

    engine = get_rag_engine()
    
    # æ¥æ”¶å…©å€‹å›å‚³å€¼ï¼šæ–‡ç»å…§å®¹å­—ä¸²ã€ä¾†æºè©³ç´°è³‡æ–™
    knowledge, sources = get_knowledge_context(engine, user_question)

    prompt = f"""
ä½ æ˜¯ä¸€ä½ç¶“é©—è±å¯Œä¸”è¦ªåˆ‡çš„ã€Œå°ç£é†«å¸«ã€ã€‚
è«‹é–±è®€ä»¥ä¸‹çš„ã€é†«ç™‚æ–‡ç»ã€‘ï¼Œä¸¦ç”¨ã€Œå°ç£ç¹é«”ä¸­æ–‡ã€å›ç­”æ‚£è€…çš„å•é¡Œã€‚

ã€å›ç­”å®ˆå‰‡ã€‘
1. **èªæ°£è‡ªç„¶**ï¼šåƒåœ¨è¨ºé–“å°è©±ä¸€æ¨£ï¼Œæº«æš–ä¸”å°ˆæ¥­ã€‚
2. **çµè«–å…ˆè¡Œ**ï¼šç¬¬ä¸€å¥è©±ç›´æ¥å›ç­”é‡é»ï¼Œæ¥è‘—è§£é‡‹åŸå› ã€‚
3. **æ¶ˆé™¤ç„¦æ…®**ï¼šçµ¦äºˆæ­£ç¢ºè§€å¿µä¸¦å®‰æ’«æƒ…ç·’ã€‚
4. **ç¦æ­¢æ©Ÿæ¢°å¼ç”¨èª**ï¼šä¸è¦èªªã€Œæ ¹æ“šè³‡æ–™ã€ï¼Œç›´æ¥å…§åŒ–æˆçŸ¥è­˜ã€‚

=== é†«ç™‚æ–‡ç» ===
{knowledge}
=== æ–‡ç»çµæŸ ===

æ‚£è€…å•é¡Œï¼š{user_question}
é†«å¸«å›ç­”ï¼š
"""
    
    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.4,
            "top_p": 0.9,
            "repetition_penalty": 1.1,
            "num_ctx": 4096
        }
    }

    try:
        print(f"ğŸ¤– [Server] å‘ Ollama ç™¼é€è«‹æ±‚: {user_question}")
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)
        response.raise_for_status()
        
        result_json = response.json()
        raw_answer = result_json.get("response", "")
        final_answer = cc.convert(raw_answer)
        
        # å›å‚³ç­”æ¡ˆèˆ‡ä¾†æºè³‡æ–™çµ¦å‰ç«¯
        return jsonify({
            "answer": final_answer,
            "sources": sources
        })

    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        return jsonify({"answer": "ç›®å‰ç³»çµ±ç¹å¿™ï¼Œè«‹æª¢æŸ¥å¾Œç«¯é€£ç·šã€‚", "sources": []}), 500

if __name__ == "__main__":
    get_rag_engine() # é ç†±æ¨¡å‹
    print("âœ… ä¼ºæœå™¨å·²ä¿®å¾©ä¸¦å•Ÿå‹•")
    app.run(host='0.0.0.0', port=5000, debug=True)
