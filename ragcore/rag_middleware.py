import time
import requests
import json
# 1. å¼•å…¥ OpenCC
from opencc import OpenCC 
from rag_core import initialize_rag_system

# ==========================================
# ğŸ”§ è¨­å®šå€
# ==========================================
WINDOWS_IP = "172.18.112.1"  # æ‚¨ç›®å‰çš„ Windows IP
OLLAMA_API_URL = f"http://{WINDOWS_IP}:11434/api/generate"
MODEL_NAME = "qwen2.5:14b"

# åˆå§‹åŒ–è½‰æ›å™¨ (s2twp = Simplified to Traditional Taiwan with Phrases)
# é€™æœƒé€£ç”¨èªéƒ½é †ä¾¿ä¿®æ­£ (ä¾‹å¦‚ï¼šä¿¡æ¯ -> è¨Šæ¯, è³ªé‡ -> å“è³ª)
cc = OpenCC('s2twp')

class RAGController:
    def __init__(self):
        print("ğŸš€ [ä¸­è½‰ç«™] ç³»çµ±å•Ÿå‹•ä¸­...")
        self.engine = initialize_rag_system()
        print(f"âœ… [ä¸­è½‰ç«™] RAG å¼•æ“æ›è¼‰å®Œæˆï¼ç›®æ¨™æ¨¡å‹: {MODEL_NAME}")

    def get_knowledge_context(self, user_question):
        # é€™è£¡ä¸è®Šï¼Œè² è²¬æ’ˆè³‡æ–™
        results = self.engine.search(user_question, k=3)
        context_str = ""
        if not results:
            return "" # æ²’è³‡æ–™å°±ç•™ç©ºï¼Œè®“ Prompt æ±ºå®šæ€éº¼å›

        for i, res in enumerate(results):
            doc = res['doc']
            score = res['score']
            if score < 0.35: continue 
            
            context_str += f"ã€åƒè€ƒæ–‡ç» {i+1}ã€‘\n"
            context_str += f"å…§å®¹: {doc['a']}\n\n"
        
        return context_str

    def ask_ollama(self, user_question):
        # 1. æª¢ç´¢è³‡æ–™
        knowledge = self.get_knowledge_context(user_question)
        
        # 2. æº–å‚™ Prompt (ğŸ”¥ é—œéµå„ªåŒ–è™•)
        # æŒ‡ä»¤é‡é»ï¼š
        # - è§’è‰²ï¼šå°ç£é†«å¸« (èªæ°£è¦ªåˆ‡ã€å°ˆæ¥­)
        # - çµæ§‹ï¼šå…ˆè¬›çµè«– -> å†è§£é‡‹åŸå›  -> æœ€å¾Œçµ¦å»ºè­°
        # - ç¦èªï¼šä¸è¦èªª "æ ¹æ“šè³‡æ–™..."
        prompt = f"""
ä½ æ˜¯ä¸€ä½ç¶“é©—è±å¯Œä¸”è¦ªåˆ‡çš„ã€Œå°ç£é†«å¸«ã€ã€‚
è«‹é–±è®€ä»¥ä¸‹çš„ã€é†«ç™‚æ–‡ç»ã€‘ï¼Œä¸¦ç”¨ã€Œå°ç£ç¹é«”ä¸­æ–‡ã€å›ç­”æ‚£è€…çš„å•é¡Œã€‚

ã€å›ç­”å®ˆå‰‡ã€‘
1. **èªæ°£è‡ªç„¶**ï¼šåƒåœ¨è¨ºé–“å°è©±ä¸€æ¨£ï¼Œæº«æš–ä¸”å°ˆæ¥­ã€‚ä¸è¦æœ‰ç¿»è­¯è…”ã€‚
2. **çµè«–å…ˆè¡Œ**ï¼šç¬¬ä¸€å¥è©±ç›´æ¥å›ç­”æ˜¯æˆ–å¦ï¼Œæ¥è‘—å†è§£é‡‹åŸå› ã€‚
3. **æ¶ˆé™¤ç„¦æ…®**ï¼šå¦‚æœæ‚£è€…çš„å•é¡Œæ¶‰åŠè¿·æ€ï¼ˆå¦‚æ€§ç—…ã€ç™Œç—‡ï¼‰ï¼Œè«‹çµ¦äºˆæ­£ç¢ºè§€å¿µä¸¦å®‰æ’«æƒ…ç·’ã€‚
4. **ç¦æ­¢æ©Ÿæ¢°å¼ç”¨èª**ï¼šä¸è¦èªªã€Œæ ¹æ“šåƒè€ƒè³‡æ–™é¡¯ç¤ºã€ã€ã€Œæ–‡ç»æåˆ°ã€ï¼Œè«‹ç›´æ¥å…§åŒ–æˆä½ çš„çŸ¥è­˜èªªå‡ºä¾†ã€‚

=== é†«ç™‚æ–‡ç» ===
{knowledge}
=== æ–‡ç»çµæŸ ===

æ‚£è€…å•é¡Œï¼š{user_question}
é†«å¸«å›ç­”ï¼š
"""
        # 3. è¨­å®šè«‹æ±‚åƒæ•¸ (å¾®èª¿ç‰ˆ)
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.4,   # ç¨å¾®æé«˜ä¸€é»é»ï¼Œè®“èªå¥æ›´æµæš¢ä¸å‘†ç‰ˆ (åŸ 0.3)
                "top_p": 0.9,         # æ ¸å–æ¨£ï¼Œè®“ç”¨è©ç¨å¾®å¤šæ¨£åŒ–
                "repetition_penalty": 1.1, # é¿å…é‡è¤‡å›‰å—¦
                "num_ctx": 4096
            }
        }

        print(f"ğŸ¤– [æ¨¡å‹] æ­£åœ¨æ€è€ƒä¸¦æ’°å¯«å»ºè­°...")
        
        try:
            response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)
            response.raise_for_status()
            
            result_json = response.json()
            raw_answer = result_json.get("response", "")
            
            # ğŸ”¥ 4. æœ€å¾Œä¸€é—œï¼šç”¨ OpenCC å¼·åˆ¶è½‰ç¹é«”
            final_answer = cc.convert(raw_answer)
            
            return final_answer

        except requests.exceptions.ConnectionError:
            return f"âŒ é€£ç·šå¤±æ•—ï¼è«‹ç¢ºèª Windows IP æ˜¯å¦è®Šæ›´æˆ–é˜²ç«ç‰†è¨­å®šã€‚"
        except Exception as e:
            return f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}"

if __name__ == "__main__":
    bot = RAGController()
    while True:
        q = input("\nè«‹è¼¸å…¥é†«ç™‚å•é¡Œ (è¼¸å…¥ q é›¢é–‹): ").strip()
        if q.lower() == 'q': break
        if not q: continue

        answer = bot.ask_ollama(q)
        print("\n" + "="*20 + " ğŸ©º AI é†«å¸«å»ºè­° " + "="*20)
        print(answer) # é€™è£¡å°å‡ºä¾†çš„å°±æœƒæ˜¯æ¼‚äº®çš„ç¹é«”ä¸­æ–‡äº†
        print("="*55)