import requests
import json
from opencc import OpenCC

# å¼•ç”¨æ‚¨åŸæœ¬çš„è¨­å®š
WINDOWS_IP = "172.18.112.1"
OLLAMA_API_URL = f"http://{WINDOWS_IP}:11434/api/generate"
MODEL_NAME = "qwen2.5:14b"
cc = OpenCC('s2twp')

class MultiTurnRAG:
    def __init__(self, rag_engine):
        self.rag_engine = rag_engine
        # ç°¡å–®çš„è¨˜æ†¶é«”æš«å­˜ï¼Œå¯¦éš›ç”Ÿç”¢ç’°å¢ƒé€šå¸¸ç”¨ Redis æˆ–è³‡æ–™åº«
        # çµæ§‹: { "user_id": [ {"role": "user", "content": "..."}, ... ] }
        self.sessions = {} 

    def get_history(self, user_id, limit=6):
        """å–å¾—æœ€è¿‘ N è¼ªå°è©±æ­·å²"""
        history = self.sessions.get(user_id, [])
        return history[-limit:] # é™åˆ¶é•·åº¦é¿å… Token çˆ†ç‚¸

    def update_history(self, user_id, role, content):
        if user_id not in self.sessions:
            self.sessions[user_id] = []
        self.sessions[user_id].append({"role": role, "content": content})

    def rewrite_query(self, user_question, history):
        """
        ã€é—œéµæ­¥é©Ÿã€‘
        åˆ©ç”¨ LLM å°‡ã€Œå¤šè¼ªå°è©±ã€ä¸­çš„ä»£è©ï¼ˆå®ƒã€é€™å€‹ã€é‚£å€‹äºº...ï¼‰
        é‚„åŸæˆå…·é«”çš„åè©ï¼Œè®Šæˆä¸€å€‹ã€Œç¨ç«‹å¯æœå°‹çš„å•é¡Œã€ã€‚
        """
        if not history:
            return user_question

        # å°‡æ­·å²è½‰ç‚ºå­—ä¸²ä¾› Prompt ä½¿ç”¨
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
        
        prompt = f"""
ä½ æ˜¯ä¸€å€‹å°è©±åˆ†æåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹çš„ã€å°è©±æ­·å²ã€‘ï¼Œå°‡ä½¿ç”¨è€…çš„ã€æœ€æ–°å•é¡Œã€‘æ”¹å¯«ç‚ºä¸€å€‹ã€Œèªæ„å®Œæ•´ä¸”ç¨ç«‹çš„å•é¡Œã€ã€‚
åªè¦è£œå…¨çœç•¥çš„ä¸»è©æˆ–é‡æ¸…ä»£åè©å³å¯ï¼Œä¸è¦å›ç­”å•é¡Œï¼Œä¹Ÿä¸è¦æ”¹è®ŠåŸæ„ã€‚

ã€å°è©±æ­·å²ã€‘
{history_str}

ã€æœ€æ–°å•é¡Œã€‘
{user_question}

ã€æ”¹å¯«å¾Œçš„ç¨ç«‹å•é¡Œã€‘ï¼š
"""
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {"temperature": 0.1} # æº«åº¦ä½ä¸€é»ï¼Œä¿æŒç²¾æº–
        }
        
        try:
            print(f"ğŸ”„ [Rewriter] æ­£åœ¨é‡å¯«å•é¡Œ: {user_question}")
            response = requests.post(OLLAMA_API_URL, json=payload, timeout=30)
            result = response.json().get("response", "").strip()
            print(f"âœ… [Rewriter] é‡å¯«çµæœ: {result}")
            return result
        except:
            print("âš ï¸ é‡å¯«å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å•é¡Œ")
            return user_question

    def process_chat(self, user_id, user_question):
        # 1. å–å¾—æ­·å²
        history = self.get_history(user_id)

        # 2. ã€é—œéµã€‘é‡å¯«å•é¡Œ (è§£æ±º "å®ƒ" æ˜¯èª°çš„å•é¡Œ)
        search_query = self.rewrite_query(user_question, history)

        # 3. ä½¿ç”¨é‡å¯«å¾Œçš„å•é¡Œå» RAG æœå°‹ (å‘¼å«æ‚¨åŸæœ¬çš„ engine)
        results = self.rag_engine.search(search_query, k=3)
        
        # æ•´ç†æª¢ç´¢çµæœ
        context_str = ""
        sources = []
        for i, res in enumerate(results):
            if res['score'] < 0.35: continue
            context_str += f"ã€æ–‡ç» {i+1}ã€‘{res['doc']['a']}\n"
            sources.append({"id": i+1, "content": res['doc']['a'], "score": round(res['score']*10, 2)})

        # 4. ç”Ÿæˆæœ€çµ‚å›ç­” (åŠ å…¥ Context + History)
        # é€™è£¡çš„ Prompt ç¨å¾®èª¿æ•´ï¼Œè®“æ¨¡å‹çŸ¥é“æœ‰æ­·å²å°è©±çš„å­˜åœ¨
        final_prompt = f"""
ä½ æ˜¯ä¸€ä½å°ç£é†«å¸«ã€‚è«‹åƒè€ƒã€æ­·å²å°è©±ã€‘èˆ‡ã€é†«ç™‚æ–‡ç»ã€‘ï¼Œå›ç­”æ‚£è€…çš„æœ€æ–°å•é¡Œã€‚

ã€æ­·å²å°è©±ã€‘
{history}

ã€é†«ç™‚æ–‡ç»ã€‘
{context_str}

ã€æ‚£è€…æœ€æ–°å•é¡Œã€‘
{user_question}

é†«å¸«å›ç­” (ç¹é«”ä¸­æ–‡ï¼Œè¦ªåˆ‡å°ˆæ¥­)ï¼š
"""
        
        payload = {
            "model": MODEL_NAME,
            "prompt": final_prompt,
            "stream": False,
            "options": {
                "temperature": 0.4,
                "num_ctx": 4096
            }
        }

        print(f"ğŸ¤– [Chat] ç”Ÿæˆæœ€çµ‚å›ç­”...")
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)
        raw_answer = response.json().get("response", "")
        final_answer = cc.convert(raw_answer)

        # 5. æ›´æ–°æ­·å²
        self.update_history(user_id, "user", user_question)
        self.update_history(user_id, "assistant", final_answer)

        return final_answer, sources
