import json
import os
import pickle
import torch
import time
import numpy as np
# ğŸ‘‡ ç›´æ¥ä½¿ç”¨ sklearn çš„æ ¸å¿ƒæœå°‹å¥—ä»¶
from sklearn.neighbors import NearestNeighbors 
from langchain_huggingface import HuggingFaceEmbeddings
# é€™è£¡æˆ‘å€‘åªç”¨å®ƒä¾†ç”Ÿæˆå‘é‡ï¼Œä¸å­˜å–
from langchain_community.vectorstores import SKLearnVectorStore
from langchain_core.documents import Document

# --- âš™ï¸ è¨­å®šå€ ---
FILE_PATHS = ['å¥åº·001_QA_ç¹é«”.json', 'é†«å­¸å•ç­”001_QA_ç¹é«”.json']
VECTOR_STORE_PATH = "medical_rag_store.pkl"

TEST_MODE = False
TEST_LIMIT = 5000 

# --- 1. ç¡¬é«”åŠ é€Ÿè¨­å®š ---
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"--------------------------------------------------")
print(f"ğŸ”¥ é‹ç®—è£ç½®: {device.upper()}")
if device == "cuda":
    print(f"ğŸš€ åµæ¸¬åˆ°é¡¯å¡: {torch.cuda.get_device_name(0)}")
else:
    print("âŒ è­¦å‘Šï¼šç›®å‰æ­£åœ¨ä½¿ç”¨ CPUï¼")
print(f"--------------------------------------------------")

print("æ­£åœ¨è¼‰å…¥ Embedding æ¨¡å‹...")
embedding_model = HuggingFaceEmbeddings(
    model_name="shibing624/text2vec-base-chinese",
    model_kwargs={'device': device},
    encode_kwargs={'batch_size': 64}
)

# --- å®šç¾©ä¸€å€‹è¼•é‡ç´šçš„æœå°‹å¼•æ“é¡åˆ¥ (ç¹é LangChain æª¢æŸ¥) ---
class MedicalSearchEngine:
    def __init__(self, texts, metadatas, embeddings, embedding_func):
        self.texts = texts
        self.metadatas = metadatas
        self.embedding_func = embedding_func
        self.embeddings_np = np.array(embeddings)
        
        # å»ºç«‹æœå°‹ç´¢å¼•
        print("   ğŸ”§ å•Ÿå‹•é«˜æ•ˆèƒ½æœå°‹å¼•æ“ (KNN)...")
        self.knn = NearestNeighbors(n_neighbors=5, metric='l2')
        self.knn.fit(self.embeddings_np)

    def search(self, query, k=5):
        # 1. æŠŠä½¿ç”¨è€…çš„å•é¡Œè½‰æˆå‘é‡
        query_emb = self.embedding_func.embed_query(query)
        query_emb_np = np.array([query_emb])
        
        # 2. é€²è¡Œæ•¸å­¸è¨ˆç®— (æ‰¾å‡ºæœ€æ¥è¿‘çš„ k å€‹)
        dists, indices = self.knn.kneighbors(query_emb_np, n_neighbors=k)
        
        # 3. æ•´ç†çµæœ
        results = []
        for dist, idx in zip(dists[0], indices[0]):
            results.append({
                "doc": self.metadatas[idx], # é€™è£¡é¢æœ‰ original_question å’Œ original_answer
                "score": dist
            })
        return results

def load_json_files(file_paths):
    documents = []
    for file_path in file_paths:
        if not os.path.exists(file_path): continue
        print(f"ğŸ“‚ æ­£åœ¨è®€å– {file_path} ...")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if TEST_MODE:
                data = data[:TEST_LIMIT]
                print(f"   âš¡ æ¸¬è©¦æ¨¡å¼ï¼šåªå–å‰ {len(data)} ç­†")
            else:
                print(f"   ğŸ“Š å…¨é‡æ¨¡å¼ï¼šè®€å–å…± {len(data)} ç­†")

            for item in data:
                q = item.get('question', '')
                a = item.get('answer', '')
                doc = Document(
                    page_content=f"å•é¡Œ: {q}\nç­”æ¡ˆ: {a}",
                    metadata={"original_question": q, "original_answer": a}
                )
                documents.append(doc)
                if TEST_MODE and len(documents) >= TEST_LIMIT: break
        except Exception: pass
        if TEST_MODE and len(documents) >= TEST_LIMIT: break
    return documents

def save_db_data(vector_db, path):
    print(f"ğŸ’¾ æ­£åœ¨æ“·å–æ•¸æ“šä¸¦å­˜æª”...")
    data = {
        "texts": vector_db._texts,
        "embeddings": vector_db._embeddings,
        "metadatas": vector_db._metadatas,
    }
    with open(path, "wb") as f:
        pickle.dump(data, f)
    size_mb = os.path.getsize(path)/1024/1024
    print(f"âœ… å­˜æª”æˆåŠŸï¼æª”æ¡ˆå¤§å°: {size_mb:.2f} MB")

def load_engine_from_file(path, embedding_func):
    print(f"ğŸ“‚ æ­£åœ¨è®€å–æ•¸æ“šæª” {path} (è«‹è€å¿ƒç­‰å¾…)...")
    with open(path, "rb") as f:
        data = pickle.load(f)
    
    text_count = len(data.get("texts", []))
    print(f"   ğŸ‘€ æª¢æŸ¥: æª”æ¡ˆä¸­åŒ…å« {text_count} ç­†è³‡æ–™")
    
    # ç›´æ¥å›å‚³æˆ‘å€‘è‡ªå®šç¾©çš„å¼•æ“ï¼Œä¸å†ä½¿ç”¨ LangChain Store
    return MedicalSearchEngine(
        texts=data["texts"],
        metadatas=data["metadatas"],
        embeddings=data["embeddings"],
        embedding_func=embedding_func
    )

# --- ä¸»ç¨‹å¼ ---
if __name__ == "__main__":
    search_engine = None
    
    # 1. å˜—è©¦è®€æª”
    if os.path.exists(VECTOR_STORE_PATH):
        print(f"âš ï¸ ç™¼ç¾èˆŠçš„å­˜æª” {VECTOR_STORE_PATH}")
        user_input = input("â“ æ˜¯å¦è¦åˆªé™¤èˆŠæª”ä¸¦é‡æ–°è·‘å…¨é‡é‹ç®—ï¼Ÿ(y/n): ")
        
        if user_input.lower() == 'y':
            print("ğŸ—‘ï¸ åˆªé™¤èˆŠæª”ï¼Œæº–å‚™é‡æ–°é‹ç®—...")
            try: os.remove(VECTOR_STORE_PATH)
            except: pass
        else:
            print("ğŸ“‚ å˜—è©¦è¼‰å…¥èˆŠæª”...")
            try:
                search_engine = load_engine_from_file(VECTOR_STORE_PATH, embedding_model)
                print("âœ… èˆŠæª”è¼‰å…¥æˆåŠŸï¼")
            except Exception as e:
                print(f"\nâŒ è®€æª”å¤±æ•—ï¼éŒ¯èª¤åŸå› : {e}")
                print("ğŸ’¡ è«‹é¸æ“‡ 'y' é‡è·‘ä¸€æ¬¡ã€‚")
                exit()

    # 2. å¦‚æœæ²’æœ‰å¼•æ“ (ä»£è¡¨éœ€è¦æ–°å»º)
    if search_engine is None:
        docs = load_json_files(FILE_PATHS)
        if not docs: exit()
            
        print(f"\nğŸ“Š æº–å‚™å°‡ {len(docs)} ç­†è³‡æ–™é€å…¥ RTX 3070 é‹ç®—...")
        print("â±ï¸ é–‹å§‹è¨ˆæ™‚...")
        start_time = time.time()
        
        # é€™è£¡å€Ÿç”¨ LangChain ä¾†åšç¬¬ä¸€æ¬¡çš„å‘é‡è¨ˆç®— (å› ç‚ºå®ƒæœ‰ batching æ¯”è¼ƒæ–¹ä¾¿)
        temp_db = SKLearnVectorStore.from_documents(
            documents=docs,
            embedding=embedding_model
        )
        
        print(f"ğŸ é‹ç®—å®Œæˆï¼è€—æ™‚: {time.time() - start_time:.2f} ç§’")
        
        # å­˜æª”
        save_db_data(temp_db, VECTOR_STORE_PATH)
        
        # è½‰æ›æˆæˆ‘å€‘çš„æœå°‹å¼•æ“
        search_engine = MedicalSearchEngine(
            texts=temp_db._texts,
            metadatas=temp_db._metadatas,
            embeddings=temp_db._embeddings,
            embedding_func=embedding_model
        )

    print("\nâœ… ç³»çµ±å°±ç·’ï¼")
    print("--------------------------------------------------")
    while True:
        try:
            query = input("\nè«‹è¼¸å…¥é†«å­¸å•é¡Œ (q é›¢é–‹): ")
            if query.lower() in ['q', 'exit']: break
            if not query.strip(): continue
            
            # ä½¿ç”¨è‡ªå®šç¾©å¼•æ“æœå°‹
            results = search_engine.search(query, k=5)
            
            print(f"\nğŸ” æœå°‹çµæœ Top 5:")
            for i, res in enumerate(results):
                doc = res["doc"]
                score = res["score"]
                print(f"\nğŸ† Top {i+1} (Score: {score:.4f})")
                print(f"â“ Q: {doc['original_question']}")
                # å®Œæ•´å°å‡ºç­”æ¡ˆï¼Œæ²’æœ‰åˆ‡ç‰‡
                print(f"ğŸ’¡ A: {doc['original_answer']}") 
                print("-" * 30)
        except KeyboardInterrupt:
            break